---
title: "time_series_analysis"
output: html_document
---

(SUMMARY HERE)


```{r setup, include=FALSE, eval = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(xts)
library(lubridate)
library(tseries)
```

##Time Series Analysis in R
Exploring daily sales data of software firm 1C. This dataset was obtained from Kaggle and is used for the final project of the Coursera "How to win a data science competition" coutse.  NOTE: I did not take this course!

First to import the data and see how it looks:


```{r import}
#train data
train <- read.csv('data/train.csv', stringsAsFactors = FALSE)

#test data & Kaggle sample sub
test <- read.csv('data/test.csv')
sample_sub <- read.csv('data/sample_submission.csv')

#additional data included
items <- read.csv('data/items.csv')
categories <- read.csv('data/item_categories.csv')
shops <- read.csv("data/shops.csv")
```

## Describe the datasets

Initial exploration to check data types, sizes, etc. 

```{r explore}
dim(train)
head(train)

dim(test)
head(test)

head(sample_sub)

```

So, looks as if we're to predict the 'item_cnt_month' based upon 'shop_id' and 'item_id'.. A few things this informs us:

Need to craft a 'daily sales'item_cnt_day' varible in the train set into a monthly sale count to have an appropriate dependant variable for model fitting. This should be done by  
  
  -Aggregate daily sales into monthy, split by shop_id and item_id.

Once this is done, we have two other variables of interest in the train:
  -date :  we'll perform some time-series analysis here
  -price: I'm, sure there is some coreelation between this and monthly sales.
  
So, first step is to create the monthly sales variable in train per shop/item. Then we can explore trends and seasonality in the time-series.

```{r set_dependent}
#First set year variable and edit typeas
train$year <- substr(train$date, 7, 10)
train$year <- as.numeric(train$year)

train$month <- substr(train$date, 4, 5)
train$month <- as.numeric(train$month)

train$date <- as.Date(train$date, "%d.%m.%Y")

sapply(train, class)
```

Should have done this first probably :/  lets check for missing values real quick.

```{r}
sapply(train, function(x) sum(is.na(x)))

```

Love Kaggle for that.  Lets divert from the train for a second and add the next block count, year, and month to the test set.  We know it's the next year/month occuring right after the end of the train set, which is october 2015.

```{r}
date <- as.Date("01.11.2015", "%d.%m.%Y")

for (i in 1:nrow(test)){
  test[i,4] = 35
  test[i, 5] = 2015
  test[i, 6] = 11
}

colnames(test)[4:6] <- c('date_block_num', 'year', 'month')

```

Lets also split out the needed data into a train set which accounts for monthly sales rather than daily.
```{r, eval = FALSE}
#train$my <- floor_date(train$date, "month")

df_train <- train %>% group_by(date_block_num, shop_id, item_id, year, month) %>% 
  summarize(item_cnt_month = sum(item_cnt_day)) 

```

##Basic Median fit

I fit fit and modeled the data based upon only the mean monthly sales per item per shop. Any NA's are filled with the median of monthly item sales throughout entire dataset... I call this the basic af model, and it's a nice one to set as a comparable to the future time-series..

```{r}
basic <- df_train %>% group_by(shop_id, item_id) %>% 
  summarize(mean_count = median(item_cnt_month)) 


overall_mean <- mean(basic$mean_count)

basic_test <- test %>% select(ID, shop_id, item_id)

basic_test <- merge(x = basic_test, y = basic, by = c("shop_id", "item_id"), all.x = TRUE)
basic_test$mean_count[is.na(basic_test$mean_count)] <- overall_mean

preds = basic_test$mean_count
sample_sub$item_cnt_month = preds

write.csv(sample_sub, file = "D://projects/kaggle/time_series/basic_af.csv", row.names = FALSE)
```

So back to work on the time series.. lets figure out how to set it up by exploring one or two specific shop-item combos.

```{r}
ts_train <- train %>% group_by(date_block_num, shop_id, item_id) %>% 
  summarize(item_cnt_month = sum(item_cnt_day))# %>%
#  mutate(ts = xts(item_cnt_month, order.by = as.Date(ts_train$date)))



### TRIAL HERE
ts_test <- subset(ts_train, ts_train$shop_id == 10 & ts_train$item_id == 1871)

ts <- rnorm(34, m = 0, sd = 0)
ts <- ts(ts, start = 2013, frequency = 12)


for (i in 1:34){
  if(i %in% ts_test$date_block_num){
    j <- which(ts_test$date_block_num == i)
    num <- ts_test[j,4]
    ts[i] <- as.numeric(num)
  }
}


plot(ts)
abline(reg = lm(ts~time(ts)))
### END HERE

```


I'm going to focus on checking the time-series for trend/seasonality first. Performing ARIMA requires stationary data. lets make sure the cycle is correct.

```{r}
cycle(ts)
```

```{r}
boxplot(ts~cycle(ts))

```

Hmmm   I'd argue that this specific shop-item combo doesn't quite need a model to accuracy predict... It's zero'd out the past two years and every month's median is 0.

Variance remains the same so no need to log.. additional,  log doesn't act nicely with 0... Our sale items are dropping, but not quite negative infinity.


```{r}
adf.test(ts, alternative = "stationary", k = 0)
adf.test(diff(ts), alternative="stationary", k=0)

acf(ts)
acf(diff(ts))
```
I'm voting on the differentiated version.  lets check PACF as well
```{r}
pacf(diff(ts))
```


Based on the plots, I think it looks like we have an ARIMA model setup of a 1 in auto-regession and 1 or 2 in moving average..  Lets go with the 2 here  so 

AR, I, MA ->(1,1,2)

```{r}
fit112 <- arima(ts, order = c(1, 1, 2))

pred112 <- predict(fit112)
ts.plot(ts, pred112$pred, lty = c(1,3))
pred112$pred
```


Aright.. its looks good.. an estimate of very  very very close to 0. Lets find a shop-item with more sales and see how this same setup does.

```{r}

```


